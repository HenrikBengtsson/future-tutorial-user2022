[["index.html", "Tutorial: An Introduction to Futureverse for Parallel Processing in R Tutorial Overview Abstract Objectives Preparing for this tutorial", " Tutorial: An Introduction to Futureverse for Parallel Processing in R Henrik Bengtsson 2022-06-20 Tutorial Overview Title: An Introduction to Futureverse for Parallel Processing in R Where: online, useR! 2022 conference (June 20-24, 2022) When: Monday 2022-06-20 07:00-10:30 UTC-0700 Length: 3.5 hours (including breaks) Instructor: Henrik Bengtsson, University of California, San Francisco, USA URL: https://user2022.r-project.org/program/tutorials/#futureverse-parallelization-in-r Event URL: https://www.accelevents.com/e/user2022/portal/workshops/260741 Video recording: https://www.youtube.com/watch?t=840&amp;v=yTwPJyRP_bc (from 0:14:00 until 3:50:30) Number of registered participants: 60 (full) Abstract In this tutorial, you will learn how to use the future framework to turn sequential R code into parallel R code with minimal effort. There are a few ways to parallelize R code. Some solutions come built-in with R (parallel package) and others are provided through R packages available on CRAN. The future framework, available on CRAN since 2015 and used by hundreds of R packages, is designed to unify and leverage common parallelization frameworks in R, to make new and existing R code faster with minimal effort by the developer. The futureverse (https://futureverse.org) allows you, as the developer, to stay with your favorite programming style. For example, future.apply provides one-to-one alternatives to base R’s apply() and lapply() functions, furrr provides alternatives to purrr’s map() functions, and doFuture provides support for using foreach’s foreach() ...%dopar% syntax. At the same time, the user can switch to a parallel backend of their choice – e.g., they can parallelize on their local machine, across multiple local or remote machines, towards the cloud, or on a job-scheduler on a high-performance computing (HPC) cluster. As a developer, you do not have to worry about which backend the user picks – your future-based code will remain the same regardless of the parallel backend. PS. We will not cover asynchronous Shiny programming using futures and promises in this tutorial. Acknowledgements: This tutorial and other work on futureverse is funded by Essential Open Source Software programme ran by the Chan Zuckerberg Initiative (CZI EOSS #4). Objectives After completing this tutorial, my hope is that you: find parallelization less magic find parallelization less intimidating feel comfortable parallelize your own R code and understand how the future framework: significantly lowers the bar to get started with parallelization helps you avoid common mistakes and issues takes care of many things you otherwise have to worry about scales and is “future” proof keeps getting improved Preparing for this tutorial R version: R (&gt;= 4.0.0) is recommended, but all of the tutorial should work with R (&gt;= 3.5.0). R 4.2.0 was released on April 22, 2022. Operating system: Linux, macOS, or MS Windows Terminal, RStudio, Rgui, R.app, RStudio Cloud, …: whichever you prefer Ahead of time, before attending the tutorial, please install the following R packages: install.packages(&quot;future&quot;) # ~ 30 secs install.packages(&quot;future.apply&quot;) # ~ 15 secs install.packages(&quot;furrr&quot;) # ~ 60 secs install.packages(&quot;foreach&quot;) # ~ 10 secs install.packages(&quot;doFuture&quot;) # ~ 15 secs install.packages(&quot;doRNG&quot;) # ~ 15 secs install.packages(&quot;plyr&quot;) # ~ 60 secs install.packages(&quot;future.callr&quot;) # ~ 30 secs install.packages(&quot;progressr&quot;) # ~ 15 secs install.packages(&quot;progress&quot;) # ~ 15 secs The time estimates are when install the package from source on a fresh Linux R setup with a 1 Gbit/s internet connection. It’s faster when installing from binaries on macOS and MS Windows. If you already have some of these installed, please make sure to they are up-to-date before starting this tutorial, i.e. update.packages() If you have any issues, please reach out for help on https://github.com/HenrikBengtsson/future-tutorial-user2022/discussions/. "],["hello-and-practicalities.html", "Hello and practicalities Practicalities Agenda Polls", " Hello and practicalities Practicalities Format: I will present and do live-coding walkthroughs. There will be no hands-on excercises or working breakout rooms. Feel free to follow along on your own computer Feel free to interrupt for questions at any time, especially so clarification questions. Think of it as a classroom tutorial! (not as an online presentation with questions only at the end) Feel free to use the Q&amp;A in the conference system to ask questions (https://www.accelevents.com/e/user2022/portal/workshops/260741). I’ll try to address them orally. Of course, if someone likes to answer a question in the Q&amp;A, please feel free to do so Rule #1: There are no stupid questions. Period. True! (In constrast, not asking about things you wonder about, is a bit silly) Help needed during our live session: Please make me aware of new questions in the Q&amp;A chat, if I miss them (I can only multitask so much) This tutorial is available on https://github.com/HenrikBengtsson/future-tutorial-user2022/ After the session, feel free to ask more questions on https://github.com/HenrikBengtsson/future-tutorial-user2022/discussions/. I’ll try my best to help you out there Agenda (10 min): Hello and practicalities (20 min): Part 1: An Overview of The Futureverse (25 min): Part 2: The core future framework, e.g. future() and value() (10 min): break (30 min): Part 3: Map-reduce APIs, e.g. future.apply, furrr, and foreach (10 min): break (35 min): Part 4: What makes futures so easy to use? (“Business as usual”) (10 min): break (20 min): Part 5: Reporting on progress updates (10 min): Part 6: Quick summary and comparison to other parallel frameworks (20 min): Part 7: Random numbers and reproducibility (20 min): Open discussion Total time: 3.5 hours (including breaks) Polls Under ‘Polls’ in Accelevents (https://www.accelevents.com/e/user2022/portal/workshops/260741), please answer the following questions. Poll #1: How much do you know about parallelization in R? I’m new to parallelization in R I’ve tried the basics (e.g. parallel or foreach), but that’s it I’ve already used the future framework and want to learn more I have a good understand about the different parallelization solutions in R Poll #2: What is your main operating system when using R? Linux macOS MS Windows Web based, e.g. RStudio Cloud Other Poll #3: How do you run R? Terminal, e.g. R RStudio Rgui (MS Windows) R.app (macOS) VSCode (e.g. vscode-R) Emacs (e.g. Emacs Speaks Statistics) Other Poll #4: Do you have access to a cluster? Yes, ad-hoc cluster of local machines Yes, ad-hoc cluster in the cloud Yes, a high-performance compute (HPC) cluster with a job scheduler Yes, something else No What does this even mean? "],["an-overview-of-the-futureverse.html", "Part 1 An Overview of The Futureverse 1.1 Why do we parallelize? 1.2 The future package (the core of it all) 1.3 Quick intro: Evaluate R in the background 1.4 Quick intro: Parallel base-R apply 1.5 Quick intro: Parallel tidyverse apply 1.6 Quick intro: Parallel foreach 1.7 What is the Futureverse?", " Part 1 An Overview of The Futureverse 1.1 Why do we parallelize? Parallel &amp; distributed processing can be used to: speed up processing (wall time) lower memory footprint (per machine) avoid data transfers (compute where data lives) other reasons, e.g. asynchronous UI/UX 1.2 The future package (the core of it all) The hexlogo for the ‘future’ package adopted from original designed by Dan LaBar A simple, unifying solution for parallel APIs “Write once, run anywhere” 100% cross platform, e.g. Linux, macOS, MS Windows Easy to install (&lt; 0.5 MiB total); install.packages(\"future\") Well tested, lots of CPU mileage, used in production Things should “just work” Design goal: keep as minimal as possible 1.3 Quick intro: Evaluate R in the background 1.3.1 Sequentially x &lt;- 7 y &lt;- slow(x) # ~1 minute z &lt;- another(x) # ~0.5 minute # all done in ~1.5 minutes 1.3.2 In parallel library(future) plan(multisession) # run things in parallel x &lt;- 7 f &lt;- future(slow(x)) # ~1 minute (in background) ‎ z &lt;- another(x) # ~0.5 minute (in current R session) y &lt;- value(f) # get background results # all done in ~1 minutes 1.4 Quick intro: Parallel base-R apply 1.4.1 Sequentially x &lt;- 1:20 y &lt;- lapply(x, slow) # ~ 20 minutes 1.4.2 In parallel library(future.apply) plan(multisession, workers = 4) x &lt;- 1:20 y &lt;- future_lapply(x, slow) # ~ 5 minutes 1.5 Quick intro: Parallel tidyverse apply 1.5.1 Sequentially library(purrr) x &lt;- 1:20 y &lt;- map(x, slow) # ~20 minutes 1.5.2 In parallel library(furrr) plan(multisession, workers = 4) x &lt;- 1:20 y &lt;- future_map(x, slow) # ~5 minutes 1.6 Quick intro: Parallel foreach 1.6.1 Sequentially library(foreach) x &lt;- 1:20 y &lt;- foreach(z = x) %do% slow(z) # ~20 minutes Comment: Technically, we want to use y &lt;- foreach(z = x) %do% local({ slow(x) }) here. 1.6.2 In parallel library(doFuture) registerDoFuture() plan(multisession, workers = 4) x &lt;- 1:20 y &lt;- foreach(z = x) %dopar% slow(z) # ~5 minutes 1.7 What is the Futureverse? A Unifying Parallelization Framework in R for Everyone Require only minimal changes to parallelize existing R code “Write once, Parallelize anywhere” Same code regardless of operating system and parallel backend Lower the bar to get started with parallelization Fewer decisions for the developer to make Stay with your favorite coding style Worry-free: globals, packages, output, warnings, errors just work Statistically sound: Built-in parallel random number generation (RNG) Correctness and reproducibilty of highest priority “Future proof”: Support any new parallel backends to come 1.7.1 Packages part of the Futureverse Core API: future Map-reduce API: future.apply furrr doFuture (used with foreach, plyr, and BiocParallel) Parallel backends: parallel (local, MPI, (remote)) future.callr (local) future.batchtools (HPC job schedulers) more to come Additional packages: progressr (progress updates, also in parallel) The first CRAN release was on 2015-06-19, but the initial seed toward building the framework was planted back in 2005. It all grew out of collaborative, real-world research needs of large-scale scientific computations in Genomics and Bioinformatics on all operating systems. 1.7.2 Who is it for? Everyone using R Users with some experience in R, but no need to be an advanced R developer Anyone who wishes to run many slow, repetive tasks Any developer who want to support parallel processing without having to worry about the details and having to maintain parallel code Anyone who wishes to set up an asynchronous Shiny app 1.7.3 Who are using it? https://www.futureverse.org/usage.html 1.7.4 What about its quality and stability? The future package on CRAN since 2015 (exactly 7 years ago) The API is stable and rarely changes Very few breaking changes since the start 225 CRAN packages rely on it (https://www.futureverse.org/statistics.html) Top-1% most downloaded package on CRAN (https://www.futureverse.org/statistics.html) Every release is well tested (https://www.futureverse.org/quality.html) I try to work closely with package developers, e.g. deprecation, or issues with design patterns 1.7.5 Support Please use: Website: https://www.futureverse.org Package help pages: https://future.futureverse.org, https://future.apply.futureverse.org, … Discussions, questions and answers: https://github.com/HenrikBengtsson/future/discussions Bug reports: https://github.com/HenrikBengtsson/future/issues 1.7.6 How to stay up-to-date Blog: https://www.futureverse.org/blog.html (feed on https://www.jottr.org/) Twitter: #RStats (often with #parallel and #HPC) "],["the-core-future-api.html", "Part 2 The core Future API 2.1 Three atomic building blocks 2.2 Choosing parallel backend 2.3 Motto and design philosophy 2.4 Demo: ggplot2 remotely", " Part 2 The core Future API 2.1 Three atomic building blocks There are three atomic building blocks that do everything we need: f &lt;- future(expr) : evaluates an expression via a future (non-blocking, if possible) r &lt;- resolved(f) : TRUE if future is resolved, otherwise FALSE (non-blocking) v &lt;- value(f) : the value of the future expression expr (blocking until resolved) 2.1.1 Mental model: The Future API decouples a regular R assignment into two parts Let’s consider a regular assignment in R: v &lt;- expr To us, this assignment is single operator, but internally it’s done in two steps: R evaluates the expression expr on the right-hand side (RHS), and assigns the resulting value to the variable v on the left-hand side (LHS). We can think of the Future API as decoupling these two steps and giving us full access to them: f &lt;- future(expr) v &lt;- value(f) This decoupling is the key feature of all parallel processing! Let’s break this down using a simple example. Consider the following, very slow, implementation of sum(); slow_sum &lt;- function(x) { sum &lt;- 0 for (kk in seq_along(x)) { sum &lt;- sum + x[kk] Sys.sleep(0.1) # emulate 0.1 second cost per addition } sum } For example, if we call: x &lt;- 1:100 v &lt;- slow_sum(x) v #&gt; [1] 5050 it takes ten seconds to complete. We can evaluate this via a future running in the background as: library(future) plan(multisession) # evaluate futures in parallel x &lt;- 1:100 f &lt;- future(slow_sum(x)) v &lt;- value(f) When we call: f &lt;- future(slow_sum(x)) then: a future is created, comprising: the R expression slow_sum(x), function slow_sum(), and integer vector x These future components are sent to a parallel worker, which starts evaluating the R expression The future() function returns immediately a reference f to the future, and before the future evaluation is completed When we call: v &lt;- value(f) then: the future asks the worker if it’s ready or not (using resolved() internally) if it is not ready, then it waits until it’s ready (blocking) when ready, the results are collected from the worker the value of the expression is returned As we saw before, there is nothing preventing us from doing other things inbetween creating the future and asking for its value, e.g. ## Create future f &lt;- future(slow_sum(x)) ## We are free to do whatever we want while future is running, e.g. z &lt;- sd(x) ## Wait for future to be done v &lt;- value(f) 2.1.2 Keep doing other things while waiting We can use the resolved() function to check whether the future is resolved or not. If not, we can choose to do other things, e.g. output a message: f &lt;- future(slow_sum(x)) while (!resolved(f)) { message(&quot;Waiting ...&quot;) Sys.sleep(1.0) } message(&quot;Done!&quot;) #&gt; Waiting ... #&gt; Waiting ... #&gt; Waiting ... #&gt; ... #&gt; Waiting ... #&gt; Done! v &lt;- value(f) v #&gt; [1] 5050 We can of course do other things than outputting messages, e.g. calculations and checking in on other futures. 2.1.3 Evaluate several things in parallel There’s nothing preventing us from launching more than one future in the background. For example, we can split the summation of x into two parts, calculate the sum of each part, and then combine the results at the end: x_head &lt;- head(x, 50) x_tail &lt;- tail(x, 50) v1 &lt;- slow_sum(x_head) ## ~5 secs (blocking) v2 &lt;- slow_sum(x_tail) ## ~5 secs (blocking) v &lt;- v1 + v2 We can do the same in parallel: f1 &lt;- future(slow_sum(x_head)) ## ~5 secs (in parallel) f2 &lt;- future(slow_sum(x_tail)) ## ~5 secs (in parallel) ## Do other things z &lt;- sd(x) v &lt;- value(f1) + value(f2) ## ready after ~5 secs We can launch as manual parallel futures as we have parallel workers, e.g. plan(multisession, workers = 8) nbrOfWorkers() #&gt; [1] 8 plan(multisession, workers = 2) nbrOfWorkers() #&gt; [1] 2 If we launch more than this, then the call to future() will block until one of the workers are free again, e.g. plan(multisession, workers = 2) nbrOfWorkers() #&gt; [1] 2 f1 &lt;- future(slow_sum(x_head)) f2 &lt;- future(slow_sum(x_tail)) f3 &lt;- future(slow_sum(1:200)) ## &lt;= blocks here resolved(f1) #&gt; [1] TRUE resolved(f2) #&gt; [1] TRUE resolved(f3) #&gt; [1] FALSE 2.2 Choosing parallel backend plan() - set how and where futures are evaluated 2.2.1 sequential (default) plan(sequential) f1 &lt;- future(slow_sum(x_head)) # blocks until done f2 &lt;- future(slow_sum(x_tail)) # blocks until done v &lt;- value(f1) + value(f2) 2.2.2 multisession: in parallel on local computer plan(multisession, workers = 2) f1 &lt;- future(slow_sum(x_head)) # in the background f2 &lt;- future(slow_sum(x_tail)) # in the background v &lt;- value(f1) + value(f2) What’s happening under the hood is(*): workers &lt;- parallelly::makeClusterPSOCK(2) plan(cluster, workers = workers) which is very similar to: workers &lt;- parallel::makePSOCKcluster(2) plan(cluster, workers = workers) (*) It actually does makeClusterPSOCK(2, rscript_libs = .libPaths()), which gives a smoother ride in some R setups, e.g. RStudio Connect. 2.2.3 cluster: in parallel on multiple computers If we have SSH access to other machines with R installed, we can do: hostnames &lt;- c(&quot;pi&quot;, &quot;remote.server.org&quot;) plan(cluster, workers = hostnames) f1 &lt;- future(slow_sum(x_head)) # on either &#39;pi&#39; or &#39;remote.server.org&#39; f2 &lt;- future(slow_sum(x_tail)) # on either &#39;pi&#39; or &#39;remote.server.org&#39; v &lt;- value(f1) + value(f2) What’s happening under the hood is: hostnames &lt;- c(&quot;pi&quot;, &quot;remote.server.org&quot;) workers &lt;- parallelly::makeClusterPSOCK(hostnames) plan(cluster, workers = workers) where makeClusterPSOCK() connects to the different machines over SSH using pre-configured SSH keys and reverse tunneling of ports. FYI, if you would try to do this with the parallel package, you need to: know your public IP number open up your incoming firewall (requires admin rights + security risk) configure port forwarding from public IP number to your local machine (requires admin rights) e.g. workers &lt;- parallel::makePSOCKcluster(hostnames, master = my_public_ip) This is one reason why the parallelly package was created - “it just works”; workers &lt;- parallelly::makeClusterPSOCK(hostnames) The ‘parallelly’ hexlogo The parallelly package is a utility package, part of the futureverse. 2.2.4 There are other parallel backends and more to come 2.2.4.1 future.callr - parallelize locally using callr The callr package can evaluate R expressions in the background on your local computer. The future.callr implements a future backend on top of callr, e.g. plan(future.callr::callr, workers = 4) This works similarly to plan(multisession, workers = 4), but has the benefit of being able to run more than 125 background workers, which is a limitation of R itself. 2.2.4.2 future.batchtools - parallelize using batchtools The batchtools package is designed to evaluate R expressions via a, so called, job scheduler. Job schedulers are commonly used on high-performance compute (HPC) clusters, where many users run at the same time. The job scheduler allows them to request slots on the system, which often has tens or hundreds of compute nodes. Common job schedulers are Slurm, SGE, and Torque. The future.batchtools implements a future backend on top of batchtools, e.g. plan(future.batchtools::batchtools_slurm) This will cause future() to be submitted to the job scheduler’s queue. When a slot is available, the job is processed on one of the many compute nodes, and when done, the results are stored to file. Calling value() will read the results back into R. This future backend has a greater latency, because everything has to be queued on a shared job queue. This backend is useful for long running futures and for the huge throughput that an HPC environment can provide. 2.3 Motto and design philosophy Maximize both the developer’s and the end-user’s control: Developer decides what to parallelize, e.g. future() User decided what parallel backend to use, e.g. plan() Rule of thumb for developers: Don’t make assumptions about the user’s R environment, e.g. you might have a fast machine with 96 CPU cores, but they might have access to a large multi-machine compute cluster with thousands of cores. So, let the user decide on the plan() and do not set it inside your functions or packages. 2.4 Demo: ggplot2 remotely library(ggplot2) library(future) plan(cluster, workers = &quot;remote.server.org&quot;) f &lt;- future({ ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point() }) gg &lt;- value(f) print(gg) "],["map-reduce-apis.html", "Part 3 Map-reduce APIs 3.1 Parallel alternatives to base-R apply functions 3.2 Parallel alternatives to purrr functions", " Part 3 Map-reduce APIs Map-reduce packages that work with the future framework: future.apply furrr foreach (with doFuture) plyr (with doFuture) BiocParallel (with doFuture) Take-home message: future.apply, furrr, foreach, plyr, and BiocParallel (from Bioconductor.org) are siblings. Their goals are the same, but they provide alternative syntax for achieving them. They’re all equally good with the same performance and limitations; use the one that you prefer. It’s no different than some people prefer to use base-R lapply(), while others prefer to use purrr::map(). 3.1 Parallel alternatives to base-R apply functions The future.apply package implements plug-and-play, parallel alternatives to base-R apply functions: The most common base-R apply functions and their parallel counterparts in the future.apply package. {#tbl-future.apply} base future.apply apply() future_apply() by() future_by() eapply() future_eapply() lapply() future_lapply() Map() future_Map() mapply() future_mapply() replicate() future_replicate() sapply() future_sapply() tapply() future_tapply() vapply() future_vapply() 3.1.1 Example: base::lapply(X, FUN) Let’s introduce another slow function that calculates the square root very slowly: slow_sqrt &lt;- function(x) { Sys.sleep(1.0) ## 1 second emulated slowness sqrt(x) } If run use this with lapply() to calculate ten values, it takes 10 seconds: X &lt;- 1:10 z &lt;- lapply(X, slow_sqrt) ## takes ~10 seconds str(z) #&gt; List of 10 #&gt; $ : num 1 #&gt; $ : num 1.41 #&gt; $ : num 1.73 #&gt; ... #&gt; $ : num 3.16 We can parallelize this using future.apply as: library(future.apply) plan(multisession, workers = 4) z &lt;- future_lapply(X, slow_sqrt) How long will this take? 3.1.1.1 Other future alternatives Here are parallel alternative for achieving identical results using furrr, and foreach, plyr, and BiocParallel, while using the doFuture adaptor. library(furrr) plan(multisession, workers = 4) z &lt;- future_map(X, slow_sqrt) library(foreach) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- foreach(x = X) %dopar% { slow_sqrt(x) } library(plyr) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- llply(X, slow_sqrt, .parallel = TRUE) library(BiocParallel) register(DoparParam(), default = TRUE) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- bplapply(X, slow_sqrt) 3.1.1.2 Non-future alternatives Here are parallel alternative for achieving identical results using the parallel package that comes built-in with R. library(parallel) options(mc.cores = 4) z &lt;- mclapply(X, slow_sqrt) library(parallel) cl &lt;- makeCluster(4) parallel::setDefaultCluster(cl) parallel::clusterExport(varlist = &quot;slow_sqrt&quot;) z &lt;- parLapply(X = X, fun = slow_sqrt) stopCluster(cl) 3.1.2 Example: base::vapply(X, FUN, FUN.VALUE) X &lt;- 1:10 z &lt;- vapply(X, slow_sqrt, FUN.VALUE = NA_real_) str(z) #&gt; num [1:10] 1 1.41 1.73 2 2.24 ... library(future.apply) plan(multisession, workers = 4) z &lt;- future_vapply(X, slow_sqrt, FUN.VALUE = NA_real_) 3.1.2.1 Other future alternatives Here are parallel alternative for achieving identical results using furrr, and foreach and plyr, while using the doFuture adaptor. library(furrr) plan(multisession, workers = 4) z &lt;- future_map_dbl(X, slow_sqrt) library(foreach) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- foreach(x = X, .combine = c) %dopar% { slow_sqrt(x) } library(plyr) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- laply(X, slow_sqrt, .parallel = TRUE) 3.1.2.2 Non-future alternatives Here are parallel alternative for achieving identical results using the parallel package that comes built-in with R. library(parallel) options(mc.cores = 4) z &lt;- mclapply(X, slow_sqrt) z &lt;- unlist(z, use.names = FALSE) library(parallel) cl &lt;- makeCluster(4) parallel::setDefaultCluster(cl) parallel::clusterExport(varlist = &quot;slow_sqrt&quot;) z &lt;- parLapply(X = X, fun = slow_sqrt) z &lt;- unlist(z, use.names = FALSE) stopCluster(cl) 3.1.3 Example: base::mapply(X, Y, FUN) X &lt;- 1:10 Y &lt;- 10:1 z &lt;- mapply(X, Y, FUN = function(x, y) { slow_sqrt(x * y) }) str(z) #&gt; [1] 3.162278 4.242641 4.898979 5.291503 5.477226 #&gt; [6] 5.477226 5.291503 4.898979 4.242641 3.162278 library(future.apply) plan(multisession, workers = 4) z &lt;- future_mapply(X, Y, FUN = function(x, y) { slow_sqrt(x * y) }) 3.1.3.1 Other future alternatives Here are parallel alternative for achieving identical results using furrr, and foreach and plyr, while using the doFuture adaptor. library(furrr) plan(multisession, workers = 4) z &lt;- future_map2_dbl(X, Y, function(x, y) { slow_sqrt(x * y) }) library(foreach) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- foreach(x = X, y = Y, .combine = c) %dopar% { slow_sqrt(x * y) } library(plyr) doFuture::registerDoFuture() plan(multisession, workers = 4) z &lt;- maply(cbind(x = X, y = Y), function(x, y) { slow_sqrt(x * y) }, .expand = FALSE) names(z) &lt;- NULL 3.1.3.2 Non-future alternatives Here are parallel alternative for achieving identical results using the parallel package that comes built-in with R. library(parallel) options(mc.cores = 4) z &lt;- mcmapply(X, Y, FUN = function(x, y) { slow_sqrt(x * y) }) library(parallel) cl &lt;- makeCluster(4) parallel::setDefaultCluster(cl) parallel::clusterExport(varlist = &quot;slow_sqrt&quot;) z &lt;- parApply(X = cbind(x = X, y = Y), MARGIN = 1L, FUN = function(row) { slow_sqrt(row[&quot;x&quot;] * row[&quot;y&quot;]) }) stopCluster(cl) 3.2 Parallel alternatives to purrr functions The hexlogo for the ‘furrr’ package designed by ?? Kuhn The furrr package, by Davis Vaughan, implements plug-and-play, parallel alternatives to purrr functions: purrr furrr imap() future_imap() imap_chr() future_imap_chr() imap_dbl() future_imap_dbl() imap_dfc() future_imap_dfc() imap_dfr() future_imap_dfr() imap_int() future_imap_int() imap_lgl() future_imap_lgl() imap_raw() future_imap_raw() invoke_map() future_invoke_map() invoke_map_chr() future_invoke_map_chr() invoke_map_dbl() future_invoke_map_dbl() invoke_map_dfc() future_invoke_map_dfc() invoke_map_dfr() future_invoke_map_dfr() invoke_map_int() future_invoke_map_int() invoke_map_lgl() future_invoke_map_lgl() invoke_map_raw() future_invoke_map_raw() iwalk() future_iwalk() map() future_map() map_at() future_map_at() map_chr() future_map_chr() map_dbl() future_map_dbl() map_dfc() future_map_dfc() map_dfr() future_map_dfr() map_if() future_map_if() map_int() future_map_int() map_lgl() future_map_lgl() map_raw() future_map_raw() map2() future_map2() map2_chr() future_map2_chr() map2_dbl() future_map2_dbl() map2_dfc() future_map2_dfc() map2_dfr() future_map2_dfr() map2_int() future_map2_int() map2_lgl() future_map2_lgl() map2_raw() future_map2_raw() modify() future_modify() modify_at() future_modify_at() modify_if() future_modify_if() pmap() future_pmap() pmap_chr() future_pmap_chr() pmap_dbl() future_pmap_dbl() pmap_dfc() future_pmap_dfc() pmap_dfr() future_pmap_dfr() pmap_int() future_pmap_int() pmap_lgl() future_pmap_lgl() pmap_raw() future_pmap_raw() pwalk() future_pwalk() walk() future_walk() walk2() future_walk2() "],["errors-and-output.html", "Part 4 Errors and output 4.1 Business as usual: Exception handling (“dealing with errors”) 4.2 Business as usual: Warnings 4.3 Business as usual: Messages 4.4 Business as usual: Standard output 4.5 Summary: All types of output is relayed 4.6 Odds and ends", " Part 4 Errors and output Whe using the future framework, it’s business as usual: Errors produced in parallel, are relayed as-is in the main R session Warnings produced in parallel, are relayed as-is in the main R session Messages produced in parallel, are relayed as-is in the main R session Any condition produced in parallel, are relayed as-is in the main R session Standard output produced in parallel, is relayed as-is in the main R session It is only the future framework that does this. Other parallel framework ignores warnings, messages, and standard output, and they throw their own custom error when they detect errors. 4.1 Business as usual: Exception handling (“dealing with errors”) Errors produced in parallel, are relayed as-is in the main R session 4.1.1 Example setup The regular sqrt() function gives a warning if we try with a negative number, e.g. sqrt(-1) #&gt; [1] NaN #&gt; Warning message: #&gt; In sqrt(-1) : NaNs produced If we want to be more strict, we could define: strict_sqrt &lt;- function(x) { if (x &lt; 0) { stop(&quot;sqrt(x) with x &lt; 0 not allowed: &quot;, x) } sqrt(x) } which gives: strict_sqrt(-1) #&gt; Error in strict_sqrt(-1) : sqrt(x) with x &lt; 0 not allowed: -1 Let’s see how this behaves with futures: f &lt;- future(strict_sqrt(-1)) message(&quot;All good this far&quot;) #&gt; All good this far v &lt;- value(f) #&gt; Error in strict_sqrt(-1) : sqrt(x) with x &lt; 0 not allowed: -1 Note how the error is signalled when we call value() - not when we call future(), which would not be possible because the latter starts the evaluation and returns immediately. Because of this, regular exception handling applies, e.g. v &lt;- tryCatch({ value(f) }, error = function(e) { message(&quot;An error occurred: &quot;, conditionMessage(e)) NA_real_ }) #&gt; An error occurred: sqrt(x) with x &lt; 0 not allowed: -1 v #&gt; [1] NA 4.1.2 Exception handling works the same with map-reduce functions Let’s see how this behaves with different map-reduce functions. If we use it with base R map-reduce functions, errors are produced like: X &lt;- -1:2 y &lt;- lapply(X, strict_sqrt) #&gt; Error in FUN(X[[i]], ...) : sqrt(x) with x &lt; 0 not allowed: -1 We get a similar behavior with future.apply: library(future.apply) plan(multisession, workers = 2) y &lt;- future_lapply(X, strict_sqrt) #&gt; Error in ...future.FUN(...future.X_jj, ...) : #&gt; sqrt(x) with x &lt; 0 not allowed: -1 and furrr: library(furrr) plan(multisession, workers = 2) y &lt;- future_map(X, strict_sqrt) #&gt; Error in ...furrr_fn(...) : sqrt(x) with x &lt; 0 not allowed: -1 Take-home message: You can treat errors the same way as when running sequentially. See Appendix Exception handling by other parallel map-reduce APIs for a comparison with other parallel solutions that does not rely on future-based, and for why the future solution is often more natural. 4.2 Business as usual: Warnings Warnings produced in parallel, are relayed as-is in the main R session When you use warning(), the warning message is signalled as a warning condition, very similar to how errors are signalled as error conditions. For example, x &lt;- -1:2 y &lt;- log(x) #&gt; Warning message: #&gt; In log(x) : NaNs produced Futures capture warning conditions, which then are re-signalled by value(), e.g. x &lt;- -1:2 f &lt;- future(log(x)) y &lt;- value(f) #&gt; Warning message: #&gt; In log(x) : NaNs produced We can suppress warning using suppressWarnings(). This works the same way regardless whether futures are used or not. For example, y &lt;- value(f) #&gt; Warning message: #&gt; In log(x) : NaNs produced suppressWarnings(y &lt;- value(f)) One can handle warning conditions using withCallingHandlers() and globalCallingHandlers() for maximum control what to do with them, but that’s beyond this tutorial. Just like errors, future map-reduce functions handles warnings consistently, e.g. library(future.apply) plan(multisession, workers = 2) X &lt;- -1:2 y &lt;- future_lapply(X, sqrt) #&gt; Warning message: #&gt; In ...future.FUN(...future.X_jj, ...) : NaNs produced str(y) #&gt; List of 4 #&gt; $ : num NaN #&gt; $ : num 0 #&gt; $ : num 1 #&gt; $ : num 1.41 Take-home message: You can treat warnings the same way as when running sequentially. See Appendix Condition handling by other parallel map-reduce APIs for a comparison with other parallel solutions. The future framework is the only solution where it works. 4.3 Business as usual: Messages Messages produced in parallel, are relayed as-is in the main R session When you use message(), the message are signalled the same way warnings and errors are signalled, and they eventually end up in the standard error (stderr) stream - not outputted to the standard output (stdout) stream. For example, z &lt;- letters[1:8] message(&quot;Number of letters: &quot;, length(z)) #&gt; Number of letters: 8 As with warnings and errors, futures capture message conditions, which then are re-signalled by value(), e.g. f &lt;- future({ z &lt;- letters[1:8] message(&quot;Number of letters: &quot;, length(z)) z }) y &lt;- value(f) #&gt; Number of letters: 8 y #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; We can suppress messages using suppressMessages(). This works the same way regardless whether futures are used or not. For example, y &lt;- value(f) #&gt; Number of letters: 8 suppressMessages(y &lt;- value(f)) Just like for warnings and errors, future map-reduce functions handles messages consistently, e.g. library(future.apply) plan(multisession, workers = 2) X &lt;- 1:3 y &lt;- future_lapply(X, function(x) message(&quot;x = &quot;, x)) x = 1 x = 2 x = 3 One can handle message conditions using withCallingHandlers() and globalCallingHandlers() for maximum control what to do with them, but that’s beyond this tutorial. Take-home message: You can treat messages the same way as when running sequentially. See Appendix Condition handling by other parallel map-reduce APIs for a comparison with other parallel solutions. The future framework is the only solution where it works. 4.4 Business as usual: Standard output Standard output produced in parallel, is relayed as-is in the main R session When you use cat(), print(), and str(), the message string is (by default) outputted to the standard output (stdout) stream. Note that this does not rely on R’s condition system, so use a completely different infrastructure than message(). For example, z &lt;- letters[1:8] cat(&quot;Number of letters:&quot;, length(z), &quot;\\n&quot;) #&gt; Number of letters: 8 When using futures, any standard output is automatically captured and re-outputted by value(), e.g. f &lt;- future({ z &lt;- letters[1:8] cat(&quot;Number of letters:&quot;, length(z), &quot;\\n&quot;) z }) y &lt;- value(f) #&gt; Number of letters: 8 y #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; Just like you can use utils::capture.output() to capture standard output from cat(), print(), str(), …, you can use it to capture the output relayed by value(). For example, stdout &lt;- capture.output({ y &lt;- value(f) }) y #&gt; [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; stdout #&gt; [1] &quot;Number of letters: 8 &quot; We can suppress standard output using capture.output(..., file = nullfile()). This works the same way regardless whether futures are used or not. For example, y &lt;- value(f) #&gt; Number of letters: 8 capture.output(y &lt;- value(f), file = nullfile()) Just like for conditions, future map-reduce functions handles standard ouput consistently, e.g. library(future.apply) plan(multisession, workers = 2) X &lt;- 1:3 y &lt;- future_lapply(X, function(x) cat(&quot;x =&quot;, x, &quot;\\n&quot;)) x = 1 x = 2 x = 3 Take-home message: You can treat output the same way as when running sequentially. See Appendix Standard output by other parallel map-reduce APIs for a comparison with other parallel solutions that does not rely on future-based. The future-based solutions are the only ones that work correctly. 4.5 Summary: All types of output is relayed Above, we’ve learned how message, warning, and error conditions are relayed. This is actually true for all other classes of conditions. We also learned that output sent to the standard output is relayed by futures. Here is a final example where we use most types of output: f &lt;- future({ cat(&quot;Hello world\\n&quot;) message(&quot;Hi there&quot;) warning(&quot;whoops!&quot;) message(&quot;Please wait ...&quot;) log(&quot;a&quot;) message(&quot;Done&quot;) }) value(f) #&gt; Hello world #&gt; Hi there #&gt; Please wait ... #&gt; Error in log(&quot;a&quot;) : non-numeric argument to mathematical function #&gt; In addition: Warning message: #&gt; In withCallingHandlers({ : whoops! 4.6 Odds and ends 4.6.1 What about standard error (stderr)? R’s support for capturing standard error (stderr) output is poor. It can be done using: capture.output(..., type = &quot;message&quot;) However, I strongly advise against using it. The reason is that capture.output(..., type = \"message\") cannot be nested, and the most recent call will always trump any existing stderr captures. That is, if you capture standard error this way and call a function that does the same, the latter hijacks all capturing. Importantly, when it returns, remaining output to standard error will no longer be captured, despite your having requested it previously. For more details, see https://github.com/HenrikBengtsson/Wishlist-for-R/issues/55. Conclusion: Always avoid capture.output(..., type = \"message\"), and never ever use it in package code! Warning: Above type = \"message\" should not be mistaken for message(). A more informative value would have been type = \"stderr\". "],["reporting-on-progress-updates.html", "Part 5 Reporting on progress updates 5.1 Basic progress updates 5.2 Progress updates in parallel 5.3 Customizing how progress is reported 5.4 Demo: Mandelbrot sets", " Part 5 Reporting on progress updates 5.1 Basic progress updates Three strokes writing three in Chinese The progressr package can be used to report on progress when using the future framework for parallelization. slow_sqrt &lt;- function(X) { p &lt;- progressr::progressor(along = X) lapply(X, function(x) { Sys.sleep(0.1) p() sqrt(x) }) } Note how we do not specify how progress is reported or rendered. We just report on where we do progress. If we call the above as-is, no progress is reported: X &lt;- 1:50 y &lt;- slow_sqrt(X) With progress reporting: progressr::handlers(global = TRUE) X &lt;- 1:50 y &lt;- slow_sqrt(X) # |==================== | 40% 5.2 Progress updates in parallel Update to use future_lapply() instead of lapply(): slow_sqrt &lt;- function(X) { p &lt;- progressr::progressor(along = X) future.apply::future_lapply(X, function(x) { Sys.sleep(0.1) p(message = sprintf(&quot;x=%g&quot;, x)) # &lt;= passing along a message sqrt(x) }) } This works the same way when running sequentially: progressr::handlers(global = TRUE) plan(sequential) X &lt;- 1:50 y &lt;- slow_sqrt(X) # |==================== | 40% and when running in parallel: progressr::handlers(global = TRUE) plan(multisession) X &lt;- 1:50 y &lt;- slow_sqrt(X) # |==================== | 40% as well as running remotely: progressr::handlers(global = TRUE) hostnames &lt;- c(&quot;pi&quot;, &quot;remote.server.org&quot;) plan(cluster, workers = hostnames) X &lt;- 1:50 y &lt;- slow_sqrt(X) # |==================== | 40% “Near-live” progress updates: Note that, even if there are only two parallel workers, and therefore two futures here, we will still recieve progress updates whilest these futures are busy processing all elements. This works because the progress updates, signalled by p() above, are sent back to our main R session using background communication channels. These channels are frequently polled (by resolved() and value()) until the futures are completed, which is how we can relay and handle these progress updates in parallel. 5.3 Customizing how progress is reported You might have noticed I’m using the term “progress updates” and “progress reporting”, rather than “progress bars”. The reason is that progress can be reported in many other ways than via visual progress bars, e.g. by audio, via the operating system’s built-in notification framework, via email, etc. Below is an example where we use the beepr package to play sounds as we progress. There are no visual cues of progress - just sound. progressr::handlers(global = TRUE) progressr::handlers(&quot;beepr&quot;) X &lt;- 1:50 y &lt;- slow_sqrt(X) # 𝅘𝅥𝅮 ♫ . . . . . . . . . ♫ We can also combine multiple ways of reporting on progress, e.g. by combining audio from beepr and progress bars from the progress package: progressr::handlers(c(&quot;beepr&quot;, &quot;progress&quot;)) and also configure them in detail: library(progressr) handlers(list( handler_progress( format = &quot;:spin :current/:total (:message) [:bar] :percent in :elapsed ETA: :eta&quot;, width = 60, complete = &quot;+&quot; ), handler_beepr( finish = &quot;wilhelm&quot; ) )) which gives: X &lt;- 1:50 y &lt;- slow_sqrt(X) # \\ 4/10 (x=32) [+++++++++&gt;-------------] 40% in 1s ETA: 1s If you use RStudio, you can report on progress via the RStudio Job interface: progressr::handlers(&quot;rstudio&quot;) 5.4 Demo: Mandelbrot sets library(future) plan(sequential) options(future.demo.mandelbrot.region = 1L) demo(&quot;mandelbrot&quot;, package = &quot;progressr&quot;, ask = FALSE) library(future) plan(multisession, workers = 4) options(future.demo.mandelbrot.region = 1L) demo(&quot;mandelbrot&quot;, package = &quot;progressr&quot;, ask = FALSE) library(future) plan(multisession, workers = 4) options(future.demo.mandelbrot.region = 2L) demo(&quot;mandelbrot&quot;, package = &quot;progressr&quot;, ask = FALSE) library(future) plan(multisession, workers = 4) options(future.demo.mandelbrot.region = 2L) options(future.demo.mandelbrot.delay = FALSE) demo(&quot;mandelbrot&quot;, package = &quot;progressr&quot;, ask = FALSE) library(future) plan(multisession, workers = 8) options(future.demo.mandelbrot.region = 3L) options(future.demo.mandelbrot.delay = FALSE) demo(&quot;mandelbrot&quot;, package = &quot;progressr&quot;, ask = FALSE) Source: https://github.com/HenrikBengtsson/progressr/blob/develop/demo/mandelbrot.R "],["quick-summary-and-comparison-to-other-parallel-frameworks.html", "Part 6 Quick summary and comparison to other parallel frameworks 6.1 Feature comparisons 6.2 Parallel-backend comparisons", " Part 6 Quick summary and comparison to other parallel frameworks 6.1 Feature comparisons future framework foreach with doFuture BiocParallel with doFuture parallel foreach* BiocParallel* Exception handling ✓ ✓ (special) ✓ (special) ✓ (special) ✓ (special) ✓ (special) Warnings ✓ ✓ ✓ no no no Messages ✓ ✓ ✓ no no no Standard output ✓ ✓ ✓ no no no Parallel RNG ✓ ✓ (special) ✓ (tedious) ✓ (manual) ✓ (special) ✓ Progress updates ✓ (near-live) ✓ ✓ (near-live) no no ✓ (per task) Single-expression parallelization ✓ (future() &amp; value()) no no ✓ (mcparallel() &amp; mcollect()) no no (*) With any other parallel backend than a future one. By “near-live” progress updates, we mean that progress is updated every time a worker is polled, which happens frequently. 6.2 Parallel-backend comparisons future framework foreach with doFuture BiocParallel with doFuture parallel foreach* BiocParallel* sequential ✓ ✓ ✓ no ✓ ✓ parallel (forked) ✓ ✓ ✓ ✓ ✓ ✓ parallel (PSOCK) ✓ ✓ ✓ ✓ ✓ ✓ parallel (&gt; 125 workers) ✓ (callr) ✓ (callr) ✓ (callr) no no no multiple machines ✓ ✓ ✓ ✓ (*) ✓ (**) ✓ (**) HPC job scheduler ✓ (batchtools) ✓ ✓ (batchtools) no no ✓ (batchtools) (*) With parallelly::makeClusterPSOCK() it is easy to set up remote parallel workers over SSH. To do the same with parallel::makePSOCKcluster(), one needs to reconfigure the firewall. (**) One can use registerDoParallel(workers) where workers &lt;- parallelly::makeClusterPSOCK(...) to use remote workers with foreach. BiocParallel can use foreach via DoparParam(). "],["random-numbers-and-reproducibility.html", "Part 7 Random numbers and reproducibility 7.1 Why is this important? 7.2 Futures use proper parallel RNG", " Part 7 Random numbers and reproducibility When we draw random number in R, e.g. x &lt;- rnorm(10) the internals of R makes sure to follow best practices so that we get numbers that have “random” properties, e.g. knowing x[1:9] does not help us predict x[10]. R uses a so called random number generator (RNG) to generate random numbers. More precisely, it uses a pseudo RNG, because 100% true random numbers are really hard to get by. An pseudo RNG is a deterministic RNG algorithm that produces a sequence of numbers that have good “random” properties. Developing good pseudo RNGs is a research field in itself, which we won’t cover in this tutorial, but what is worth mentioning is that R support several different kinds of RNGs, and the default one has carefully been chosen for us; &gt; RNGkind() [1] &quot;Mersenne-Twister&quot; &quot;Inversion&quot; &quot;Rejection&quot; We can get reproducible random numbers by setting the RNG seed, e.g. set.seed(42) rnorm(4) #&gt; [1] 1.3709584 -0.5646982 0.3631284 0.6328626 rnorm(4) #&gt; [1] 0.40426832 -0.10612452 1.51152200 -0.09465904 If we redo the above, we get an identical sequence of “random” numbers: set.seed(42) rnorm(4) #&gt; [1] 1.3709584 -0.5646982 0.3631284 0.6328626 rnorm(4) #&gt; [1] 0.40426832 -0.10612452 1.51152200 -0.09465904 7.1 Why is this important? Random number generation (RNG) is important, because: many statistical methods rely on it for correctness, science rely on it for reproducibility. If RNG produce “poor” random numbers, there is a risk we get invalid results. When performing calculations and analyses in parallel, we cannot use the default RNG (Mersenne-Twister). If we do, there is a risk that we end up with correlated random numbers across parallel workers, which then results in biased results and incorrect conclusions. To solve this, other pseudo RNGs have been developed specifically for parallel processing: L’Ecuyer is a RNG that works well for parallel processing &gt; RNGkind(&quot;L&#39;Ecuyer-CMRG&quot;) &gt; RNGkind() [1] &quot;L&#39;Ecuyer-CMRG&quot; &quot;Inversion&quot; &quot;Rejection&quot; 7.2 Futures use proper parallel RNG The future framework uses the L’Ecuyer parallel RNG. It’s all taken care of automatically. All you need to remember is to declare that your parallel code uses random numbers. This you can do by specifying seed = TRUE, e.g. f &lt;- future(rnorm(4), seed = TRUE) Futures respects the random seed, and can therefore produce reproducible random numbers, e.g. set.seed(42) f &lt;- future(rnorm(4), seed = TRUE) value(f) #&gt; [1] -1.6430042 -1.0003081 -0.3619149 0.1475366 set.seed(42) f &lt;- future(rnorm(4), seed = TRUE) value(f) #&gt; [1] -1.6430042 -1.0003081 -0.3619149 0.1475366 Comment: The random numbers produced with the same seed (here 42) are not identical when using the L’Ecuyer RNG as when using the Mersenne-Twister RNG. 7.2.1 What happens if you forget to declare seed = TRUE? If you don’t specify seed = TRUE, and your future code end up using random numbers, then the future framework detects this and produces an informative wwarning that is hard to miss: f &lt;- future(rnorm(4)) x &lt;- value(f) #&gt; Warning message: #&gt; UNRELIABLE VALUE: Future (&#39;&lt;none&gt;&#39;) unexpectedly generated random numbers without specifying argument &#39;seed&#39;. There is a risk that those random numbers are not statistically sound and the overall results might be invalid. To fix this, specify &#39;seed=TRUE&#39;. This ensures that proper, parallel-safe random numbers are produced via the L&#39;Ecuyer-CMRG method. To disable this check, use &#39;seed=NULL&#39;, or set option &#39;future.rng.onMisuse&#39; to &quot;ignore&quot;. 7.2.2 Random numbers with parallel map-reduce functions The future.apply package use argument future.seed = TRUE, and the furrr package argument .options = furrr_options(seed = TRUE) for declaring that RNG is used. For example, library(future.apply) plan(multisession, workers = 2) X &lt;- 2:4 y &lt;- future_lapply(X, rnorm, future.seed = TRUE) str(y) #&gt; List of 3 #&gt; $ : num [1:2] -0.0265 -1.7324 #&gt; $ : num [1:3] 2.5456 -0.6127 -0.0912 #&gt; $ : num [1:4] -2.107 -1.304 0.229 0.775 It does not matter what parallel backend you use, or number of parallel workers, the future framework guarantees numerically identical random numbers with the same random seed, e.g. library(future.apply) X &lt;- 2:4 plan(sequential) set.seed(42) y &lt;- future_lapply(X, rnorm, future.seed = TRUE) str(y) #&gt; List of 3 #&gt; $ : num [1:2] -0.169 -1.642 #&gt; $ : num [1:3] 0.649 1.193 0.542 #&gt; $ : num [1:4] 1.36 -1.85 -1.28 0.42 plan(multisession, workers = 4) set.seed(42) y &lt;- future_lapply(X, rnorm, future.seed = TRUE) str(y) #&gt; List of 3 #&gt; $ : num [1:2] -0.169 -1.642 #&gt; $ : num [1:3] 0.649 1.193 0.542 #&gt; $ : num [1:4] 1.36 -1.85 -1.28 0.42 plan(cluster, workers = c(&quot;pi&quot;, &quot;remote.server.org&quot;)) set.seed(42) y &lt;- future_lapply(X, rnorm, future.seed = TRUE) str(y) #&gt; List of 3 #&gt; $ : num [1:2] -0.169 -1.642 #&gt; $ : num [1:3] 0.649 1.193 0.542 #&gt; $ : num [1:4] 1.36 -1.85 -1.28 0.42 As before, if you forget to declare your need for RNG, you’ll get informative warnings about it; y &lt;- future_lapply(X, rnorm) Warning message: UNRELIABLE VALUE: One of the &#39;future.apply&#39; iterations (&#39;future_lapply-1&#39;) unexpectedly generated random numbers without declaring so. There is a risk that those random numbers are not statistically sound and the overall results might be invalid. To fix this, specify &#39;future.seed=TRUE&#39;. This ensures that proper, parallel-safe random numbers are produced via the L&#39;Ecuyer-CMRG method. To disable this check, use &#39;future.seed = NULL&#39;, or set option &#39;future.rng.onMisuse&#39; to &quot;ignore&quot;. "],["open-discussion.html", "Open discussion", " Open discussion "],["appendix.html", "Part 8 Appendix 8.1 Appendix: Exception handling by other parallel map-reduce APIs 8.2 Appendix: Condition handling by other parallel map-reduce APIs 8.3 Appendix: Standard output by other parallel map-reduce APIs 8.4 Appendix: Not everything can be parallelized 8.5 Appendix: Careful with forked parallization 8.6 Appendix: Missing globals 8.7 Appendix: Don’t assign to global environment 8.8 Appendix: foreach() is not a for-loop 8.9 Appendix: Debugging", " Part 8 Appendix 8.1 Appendix: Exception handling by other parallel map-reduce APIs The regular sqrt() function gives a warning if we try with a negative number, e.g. sqrt(-1) #&gt; [1] NaN #&gt; Warning message: #&gt; In sqrt(-1) : NaNs produced If we want to be more strict, we can define: strict_sqrt &lt;- function(x) { if (x &lt; 0) { stop(errorCondition( paste(&quot;sqrt(x) with x &lt; 0 not allowed:&quot;, x), call = sys.call(), class = &quot;strict_error&quot;) ) } sqrt(x) } which gives: strict_sqrt(-1) #&gt; Error in strict_sqrt(-1) : sqrt(x) with x &lt; 0 not allowed: -1 Let’s see how this behaves with different map-reduce functions. Our reference, base-R lapply(): X &lt;- -1:2 y &lt;- lapply(X, strict_sqrt) #&gt; Error in FUN(X[[i]], ...) : sqrt(x) with x &lt; 0 not allowed: -1 Details: tryCatch(y &lt;- lapply(X, strict_sqrt), error = str) #&gt; List of 2 #&gt; $ message: chr &quot;sqrt(x) with x &lt; 0 not allowed: -1&quot; #&gt; $ call : language FUN(X[[i]], ...) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;strict_error&quot; &quot;error&quot; &quot;condition&quot; future.apply (as expected): library(future.apply) plan(multisession, workers = 2) X &lt;- -1:2 y &lt;- future_lapply(X, strict_sqrt) #&gt; Error in ...future.FUN(...future.X_jj, ...) : #&gt; sqrt(x) with x &lt; 0 not allowed: -1 Details: tryCatch(y &lt;- future_lapply(X, strict_sqrt), error = str) #&gt; List of 2 #&gt; $ message: chr &quot;sqrt(x) with x &lt; 0 not allowed: -1&quot; #&gt; $ call : language ...future.FUN(...future.X_jj, ...) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;strict_error&quot; &quot;error&quot; &quot;condition&quot; Note how the error message is preserved and also the error class (strict_error). furrr (as expected): library(furrr) plan(multisession, workers = 2) y &lt;- future_map(X, strict_sqrt) #&gt; Error in ...furrr_fn(...) : sqrt(x) with x &lt; 0 not allowed: -1 Details: tryCatch(y &lt;- future_map(X, strict_sqrt), error = str) #&gt; List of 2 #&gt; $ message: chr &quot;sqrt(x) with x &lt; 0 not allowed: -1&quot; #&gt; $ call : language ...furrr_fn(...) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;strict_error&quot; &quot;error&quot; &quot;condition&quot; Note how the error message is preserved and also the error class (strict_error). In contrast, parallel and parLapply(): library(parallel) cl &lt;- makePSOCKcluster(2) clusterExport(cl, &quot;strict_sqrt&quot;) y &lt;- parLapply(X, strict_sqrt, cl = cl) #&gt; Error in checkForRemoteErrors(val) : #&gt; one node produced an error: sqrt(x) with x &lt; 0 not allowed: -1 Details: tryCatch(y &lt;- parLapply(X, strict_sqrt, cl = workers), error = str) #&gt; List of 2 #&gt; $ message: chr &quot;one node produced an error: sqrt(x) with x &lt; 0 not allowed: -1&quot; #&gt; $ call : language checkForRemoteErrors(val) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; Note how: the error message has changed we lost the information on error class, i.e. strict_error parallel and mclapply(): library(parallel) options(mc.cores = 2) y &lt;- parallel::mclapply(X, strict_sqrt) #&gt; Warning message: #&gt; In parallel::mclapply(X, strict_sqrt) : #&gt; all scheduled cores encountered errors in user code Here we didn’t even get an error - only a warning. We have to inspect the results to detect errors, e.g. str(y) #&gt; List of 4 #&gt; $ : &#39;try-error&#39; chr &quot;Error in FUN(X[[i]], ...) : sqrt(x) with x &lt; 0 not allowed: -1\\n&quot; #&gt; ..- attr(*, &quot;condition&quot;)=List of 2 #&gt; .. ..$ message: chr &quot;sqrt(x) with x &lt; 0 not allowed: -1&quot; #&gt; .. ..$ call : language FUN(X[[i]], ...) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;strict_error&quot; &quot;error&quot; &quot;condition&quot; #&gt; $ : num 0 #&gt; $ : &#39;try-error&#39; chr &quot;Error in FUN(X[[i]], ...) : sqrt(x) with x &lt; 0 not allowed: -1\\n&quot; #&gt; ..- attr(*, &quot;condition&quot;)=List of 2 #&gt; .. ..$ message: chr &quot;sqrt(x) with x &lt; 0 not allowed: -1&quot; #&gt; .. ..$ call : language FUN(X[[i]], ...) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;strict_error&quot; &quot;error&quot; &quot;condition&quot; #&gt; $ : num 1.41 Do detect the errors, we have to scan the results; ## Check for errors is_error &lt;- vapply(y, inherits, &quot;try-error&quot;, FUN.VALUE = NA) if (any(is_error)) { ## error objects are stored in attributes first_error &lt;- attr(y[is_error][[1]], &quot;condition&quot;) stop(&quot;Detected one or more errors: &quot;, conditionMessage(first_error)) } This a low-level, powerful feature, but it adds lots of friction and requires much more work to make sure things are correct. We cannot just use the value of mclapply(...) as-is, but we need to postprocess it to make sure we catch any errors and handle them correctly. Also: Note how elements X[2] and X[4] where processed successfully. This is because they were process together on one of the parallel workers. In contrast, X[1] and X[3] where processed by another worker, both together as lapply(X[c(1,3)], strict_sqrt), which results in a “combined” for both. Confusing? Yes! foreach: library(foreach) doFuture::registerDoFuture() plan(multisession, workers = 2) y &lt;- foreach(x = X) %dopar% strict_sqrt(x) #&gt; Error in { : task 1 failed - &quot;sqrt(x) with x &lt; 0 not allowed: -1&quot; Just like with parallel::parLapply(), we lose important information on the error: tryCatch(y &lt;- foreach(x = X) %dopar% strict_sqrt(x), error = str) #&gt; List of 2 #&gt; $ message: chr &quot;task 1 failed - \\&quot;sqrt(x) with x &lt; 0 not allowed: -1\\&quot;&quot; #&gt; $ call : language { doFuture::registerDoFuture() ... #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; Note how: the error message has changed we lost the information on error class, i.e. strict_error This is per design of %dopar% of the foreach package. 8.2 Appendix: Condition handling by other parallel map-reduce APIs Here will use warnings to illustrate how conditions are handled by the future framework, and how none of the other parallel frameworks handles them. However, everything in this section apply also to messages, and any other non-error condition type signalled by R. The regular sqrt() function gives a warning if we try with a negative number. Let’s see how this behaves with different map-reduce functions. Our reference, base-R lapply(): X &lt;- -1:2 y &lt;- lapply(X, sqrt) #&gt; Warning message: #&gt; In FUN(X[[i]], ...) : NaNs produced Details: tryCatch(y &lt;- lapply(X, sqrt), warning = str) #&gt; List of 2 #&gt; $ message: chr &quot;NaNs produced&quot; #&gt; $ call : language FUN(X[[i]], ...) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleWarning&quot; &quot;warning&quot; &quot;condition&quot; future.apply (works as expected): library(future.apply) plan(multisession, workers = 2) X &lt;- -1:2 y &lt;- future_lapply(X, sqrt) #&gt; Warning message: #&gt; In ...future.FUN(...future.X_jj, ...) : NaNs produced Details: tryCatch(y &lt;- future_lapply(X, sqrt), warning = str) #&gt; List of 2 #&gt; $ message: chr &quot;NaNs produced&quot; #&gt; $ call : language ...future.FUN(...future.X_jj, ...) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleWarning&quot; &quot;warning&quot; &quot;condition&quot; furrr (works as expected): library(furrr) plan(multisession, workers = 2) X &lt;- -1:2 y &lt;- future_map(X, sqrt) #&gt; Warning message: #&gt; In .Primitive(&quot;sqrt&quot;)(x) : NaNs produced Details: tryCatch(y &lt;- future_map(X, sqrt), warning = str) #&gt; List of 2 #&gt; $ message: chr &quot;NaNs produced&quot; #&gt; $ call : language .Primitive(&quot;sqrt&quot;)(x) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleWarning&quot; &quot;warning&quot; &quot;condition&quot; foreach with doFuture (works as expected): library(foreach) doFuture::registerDoFuture() plan(multisession, workers = 2) X &lt;- -1:2 y &lt;- foreach(x = X) %dopar% sqrt(x) #&gt; Warning message: #&gt; In sqrt(x) : NaNs produced tryCatch(y &lt;- foreach(x = X) %dopar% sqrt(x), warning = str) #&gt; List of 2 #&gt; $ message: chr &quot;NaNs produced&quot; #&gt; $ call : language sqrt(x) #&gt; - attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleWarning&quot; &quot;warning&quot; &quot;condition&quot; foreach with doParallel (does not work): library(doParallel) cl &lt;- parallel::makeCluster(2) registerDoParallel(cl) X &lt;- -1:2 y &lt;- foreach(x = X) %dopar% sqrt(x) Note, warnings are not signalled. foreach with doMC (does not work): library(doMC) registerDoMC(2) X &lt;- -1:2 y &lt;- foreach(x = X) %dopar% sqrt(x) Note, warnings are not signalled. parallel and parLapply() (does not work): library(parallel) cl &lt;- makePSOCKcluster(2) X &lt;- -1:2 y &lt;- parLapply(X, sqrt, cl = cl) Note, warnings are not signalled. parallel and mclapply() (does not work): library(parallel) options(mc.cores = 2) X &lt;- -1:2 y &lt;- mclapply(X, sqrt) Note, warnings are not signalled. 8.3 Appendix: Standard output by other parallel map-reduce APIs TL;DR: It’s only the future framework that captures and relays standard output in the main R session. Our reference, base-R lapply(): X &lt;- 1:3 void &lt;- lapply(X, print) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 output &lt;- capture.output(void &lt;- lapply(X, print)) output #&gt; [1] &quot;[1] 1&quot; &quot;[1] 2&quot; &quot;[1] 3&quot; future.apply (works as expected): library(future.apply) plan(multisession, workers = 2) X &lt;- 1:3 void &lt;- future_lapply(X, print) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 output &lt;- capture.output(void &lt;- future_lapply(X, print)) output #&gt; [1] &quot;[1] 1&quot; &quot;[1] 2&quot; &quot;[1] 3&quot; furrr (works as expected): library(furrr) plan(multisession, workers = 2) X &lt;- 1:3 void &lt;- future_map(X, print) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 output &lt;- capture.output(void &lt;- future_map(X, print)) output #&gt; [1] &quot;[1] 1&quot; &quot;[1] 2&quot; &quot;[1] 3&quot; foreach w/ doFuture (works as expected): library(doFuture) registerDoFuture() plan(multisession, workers = 2) X &lt;- 1:3 void &lt;- foreach(x = X) %dopar% print(x) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 output &lt;- capture.output({ void &lt;- foreach(x = X) %dopar% print(x) }) output #&gt; [1] &quot;[1] 1&quot; &quot;[1] 2&quot; &quot;[1] 3&quot; foreach w/ doParallel (doesn’t work): library(doParallel) cl &lt;- parallel::makeCluster(2) registerDoParallel(cl) X &lt;- 1:3 void &lt;- foreach(x = X) %dopar% print(x) output &lt;- capture.output({ void &lt;- foreach(x = X) %dopar% print(x) }) output #&gt; character(0) foreach w/ doMC (doesn’t work): library(doMC) registerDoMC(2) X &lt;- 1:3 void &lt;- foreach(x = X) %dopar% print(x) #&gt; [1] 1 #&gt; [1] 3 #&gt; [1] 2 We did get some output, but not in order. As we will see next, it’s actually only output to the same terminal but not to the R main session. If you run this in RStudio, you may not see anything. This means, there is nothing to capture; output &lt;- capture.output({ void &lt;- foreach(x = X) %dopar% print(x) }) output #&gt; character(0) Thus, the output we see above, does not end up in our main R session. parallel and mclapply() (doesn’t work): library(parallel) options(mc.cores = 2) X &lt;- 1:3 void &lt;- mclapply(X, print) #&gt; [1] 1 #&gt; [1] 3 #&gt; [1] 2 We did get some output, but not in order. As we will see next, it’s actually only output to the same terminal but not to the R main session. If you run this in RStudio, you may not see anything. This means, there is nothing to capture; output &lt;- capture.output({ void &lt;- mclapply(X, print) }) output #&gt; character(0) Thus, the output we see above, does not end up in our main R session. parallel and parLapply() (doesn’t work): library(parallel) cl &lt;- makePSOCKcluster(2) X &lt;- 1:3 void &lt;- parLapply(X, print, cl = cl) No output, and nothing to capture; output &lt;- capture.output({ void &lt;- parLapply(X, print, cl = cl) }) output #&gt; character(0) What about the outfile = \"\" trick? library(parallel) cl &lt;- makePSOCKcluster(2, outfile = &quot;&quot;) X &lt;- 1:3 void &lt;- parLapply(X, print, cl = cl) #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 It turns out also this only outputs to the terminal in which R is running. If you run this in RStudio, you may not see anything. This means, there is nothing to capture; output &lt;- capture.output({ void &lt;- parLapply(X, print, cl = workers) }) output #&gt; character(0) 8.4 Appendix: Not everything can be parallelized As explained in https://future.futureverse.org/articles/future-4-non-exportable-objects.html, not all types of objects can be sent to parallel workers. Some objects only work in the R process they were first created in. Below are example of object types from different R packages that cannot be exported to, or returned from, parallel workers. Package Examples of non-exportable types or classes base connection (externalptr) DBI DBIConnection (externalptr) inline CFunc (externalptr of class DLLHandle) keras keras.engine.sequential.Sequential (externalptr) magick magick-image (externalptr) ncdf4 ncdf4 (custom reference; non-detectable) parallel cluster and cluster nodes (connection) raster RasterLayer (externalptr; not all) Rcpp NativeSymbol (externalptr) reticulate python.builtin.function (externalptr), python.builtin.module (externalptr) rJava jclassName (externalptr) ShortRead FastqFile, FastqStreamer, FastqStreamerList (connection) sparklyr tbl_spark (externalptr) terra SpatRaster, SpatVector (externalptr) udpipe udpipe_model (externalptr) xgboost xgb.DMatrix (externalptr) xml2 xml_document (externalptr) 8.4.1 Example: R connections can be exported to parallel workers library(future) plan(multisession, workers = 2) file &lt;- tempfile() con &lt;- file(file, open = &quot;wb&quot;) cat(&quot;hello\\n&quot;, file = con) readLines(file) #&gt; [1] &quot;hello&quot; f &lt;- future({ cat(&quot;world\\n&quot;, file = con); 42 }) v &lt;- value(f) readLines(file) #&gt; [1] &quot;hello&quot; # &lt;= Huh, where did &#39;world&#39; end up?!? It turns out that we are actually silently writing to another R connection on the parallel worker with the same connection index as our temporary file: as.integer(con) #&gt; [1] 3 &gt; showConnections() description class mode text isopen 3 &quot;/tmp/hb/RtmpZMAQG0/file1ec6b03b87be50&quot; &quot;file&quot; &quot;wb&quot; &quot;binary&quot; &quot;opened&quot; 4 &quot;&lt;-localhost:11362&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; 5 &quot;&lt;-localhost:11362&quot; &quot;sockconn&quot; &quot;a+b&quot; &quot;binary&quot; &quot;opened&quot; can read can write 3 &quot;no&quot; &quot;yes&quot; 4 &quot;yes&quot; &quot;yes&quot; 5 &quot;yes&quot; &quot;yes&quot; This is really bad, because we might end up overwriting another file. This is a limitation in R. Here, R should ideally detect this and give an error. If we create yet another connection, with a higher connection index: con2 &lt;- file(file, open = &quot;wb&quot;) as.integer(con2) #&gt; [1] 6 and try to use that in parallel, we get: f &lt;- future({ cat(&quot;world\\n&quot;, file = con2); 42 }) v &lt;- value(f) #&gt; Error in cat(&quot;world\\n&quot;, file = con2) : invalid connection This is because there is no connection on the worker with index 6. Either, this is not good. This is a problem for all parallelization frameworks in R. There’s no solution to this. However, for troubleshooting, we can ask the future framework to look for non-exportable objects: options(future.globals.onReference = &quot;error&quot;) ‎ f &lt;- future({ cat(&quot;world&quot;, file = con2); 42 }) Error: Detected a non-exportable reference (&#39;externalptr&#39;) in one of the globals (&#39;con2&#39; of class &#39;file&#39;) used in the future expression Note how the problem was detected when creating the future. This prevents damage from happening. This option is disabled by default, because: there are some false positive, and check is expensive 8.4.2 Example: xml2 objects cannot be exported library(xml2) xml &lt;- read_xml(&quot;&lt;body&gt;&lt;/body&gt;&quot;) f &lt;- future({ xml_children(xml) }) value(f) ## Error: external pointer is not valid str(xml) ## List of 2 ## $ node:&lt;externalptr&gt; ## $ doc :&lt;externalptr&gt; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;xml_document&quot; &quot;xml_node&quot; As before, we can set an R options for future to look for and report on these problems before trying to parallelize: library(future) options(future.globals.onReference = &quot;error&quot;) library(xml2) xml &lt;- read_xml(&quot;&lt;body&gt;&lt;/body&gt;&quot;) f &lt;- future({ xml_children(xml) }) Error: Detected a non-exportable reference (&#39;externalptr&#39;) in one of the globals (&#39;xml&#39; of class &#39;xml_document&#39;) used in the future expression 8.5 Appendix: Careful with forked parallization The parallel::mclapply() function relies on forked parallel processing provided by the operating system; It works only on Linux and macOS It does not work on MS Windows, where it falls back to a regular lapply() call Because it uses forks, parallel::mclapply() is extremely easy to use. For example, you never have to worry about global variables. “It just works!” The corresponding future backend is multicore, which use the same underlying code base as mclapply(). In other words, using: library(future.apply) plan(multicore, workers = 4) X &lt;- 1:100 z &lt;- future_lapply(X, FUN = slow_sqrt) is almost the same as using: library(parallel) options(mc.cores = 4) X &lt;- 1:100 z &lt;- mclapply(X, FUN = slow_sqrt) but with the all the other benefits that comes with the future framework, e.g. errors, warnings, output, and random number generation. However, it is not always safe to use forked parallelization; you really need to know when and when not to use it, which is complicated. Simon Urbanek, author of mclapply() and R Core member, wrote: “Do NOT use mcparallel() in packages except as a non-default option that user can set … Multicore is intended for HPC applications that need to use many cores for computing-heavy jobs, but it does not play well with RStudio and more importantly you [as the developer] don’t know the resource available so only the user can tell you when it’s safe to use.” In other words, you can only use it reliably in code that you have 100% control over, which is rarely the case, especially not for package authors. 8.6 Appendix: Missing globals Objects that needs to be exported to parallel workers are called “globals”. They are identified by automatically code inspection. This works most of the time, but there are cases were it might fail. For example, consider: a &lt;- 1:3 b &lt;- 4:7 c &lt;- 3:5 my_sum &lt;- function(var) { sum(get(var)) } z &lt;- my_sum(var = &quot;a&quot;) In this case, it is impossible for R to know upfront that var = \"a\" is going to be used to retrieve the value of a global variable. Because of this, calling: f &lt;- future(my_sum(var = &quot;a&quot;)) z &lt;- value(f) #&gt; Error in get(var) : object &#39;a&#39; not found fails. The solution is to guide the future framework to identify a as a global variable. We can do this by adding a dummy use of a, e.g. f &lt;- future({ a ## fake use of &#39;a&#39; my_sum(var = &quot;a&quot;) }) z &lt;- value(f) 8.6.1 Example: glue::glue() - object not found Here’s another, more common example: library(glue) a &lt;- 42 s &lt;- glue(&quot;The value of a is {a}.&quot;) s #&gt; The value of a is 42. If we run this in parallel as-is, the future framework won’t be able to identify a as a needed object; library(glue) library(future) plan(multisession) a &lt;- 42 f &lt;- future(glue(&quot;The value of a is {a}.&quot;)) s &lt;- value(f) Error in eval(parse(text = text, keep.source = FALSE), envir) : object &#39;a&#39; not found As before, we can workaround it by: f &lt;- future({ a ## fake use of &#39;a&#39; glue(&quot;The value of a is {a}.&quot;) }) s &lt;- value(f) s #&gt; The value of a is 42. 8.6.2 Example: do.call() Function do.call() can be used to call a function with a set of arguments. For example, fcn &lt;- sum z &lt;- do.call(fcn, args = list(1:10)) z #&gt; [1] 55 calls fcn(1:10) == sum(1:10). This works in parallel too: library(future) plan(multisession) fcn &lt;- sum f &lt;- future(do.call(fcn, args = list(1:10))) z &lt;- value(f) z #&gt; [1] 55 As an alternative to a function, do.call() also takes the name of a function as input. For example, we can also do: fcn &lt;- sum z &lt;- do.call(&quot;fcn&quot;, args = list(1:10)) z #&gt; [1] 55 However, this is like the problem of using get(), as explained above. If we try this in parallel, we get: library(future) plan(multisession) fcn &lt;- sum f &lt;- future(do.call(&quot;fcn&quot;, args = list(1:10))) z &lt;- value(f) #&gt; Error in fcn(1:10) : could not find function &quot;fcn&quot; We could declare by adding a dummy fcn, but it’s much better to never pass the name of a function (\"fcn\") to do.call(); it’s always much better to pass the function object (fcn) itself. The same is true for apply functions, e.g. use: z &lt;- lapply(1:10, FUN = sum) but avoid: z &lt;- lapply(1:10, FUN = &quot;sum&quot;) 8.7 Appendix: Don’t assign to global environment Assigning to variable outside of a function or in the global environments (e.g. &lt;&lt;- or assign()) does not work when running in parallel. However, it extremely rare you need to do that. Instead, If you find yourself using &lt;&lt;-, it’s a strong hint that you should approach you problem in a different way! For example, if you find yourself turning: res &lt;- list() for (ii in 1:3) { res[[ii]] &lt;- letters[ii] } into: res &lt;- list() lapply(1:3, FUN = function(ii) { res[[ii]] &lt;&lt;- letters[ii] }) then you should stop and think. The correct solution is: res &lt;- lapply(1:3, FUN = function(ii) { letters[ii] }) This can easily be parallelize by replace lapply() with future_lapply() from the future.apply package. 8.8 Appendix: foreach() is not a for-loop An common example of the problem explained in Appendix: Don’t assign to global environment happens when using foreach to turn a for-loop into a foreach() call. As before, if you find yourself needing to use &lt;&lt;- in order to turn: res &lt;- list() for (ii in 1:3) { res[[ii]] &lt;- letters[ii] } into library(doFuture) registerDoFuture() plan(multisession) res &lt;- list() foreach(ii = 1:3) %dopar% { res[[ii]] &lt;&lt;- letters[ii] } then the &lt;&lt;- is a strong indication that this should not be done and it won’t work, especially when running in parallel. If you try to run the above in parallel, you will get: Error in { : task 1 failed - &quot;object &#39;res&#39; not found&quot; If you try to export res, e.g. res &lt;- list() foreach(ii = 1:3, .export = &quot;res&quot;) %dopar% { res[[ii]] &lt;&lt;- letters[ii] } you’ll find that res is not populated; res #&gt; list() The mistake is believing that foreach() is a replacement to a for-loop. It is not. Repeat after me: foreach() %dopar% { ... } is not a for-loop! foreach() %dopar% { ... } is not a for-loop! foreach() %dopar% { ... } is not a for-loop! Don’t feel bad if you thought this - you’re not alone, not the first and not the last person to think this. It’s a very common misconception and it’s the name that makes it so tempting to believe it. Instead, foreach() is much closer to an lapply() call; foreach() %dopar% { ... } is just like lapply() or future_lapply() foreach() %dopar% { ... } is just like lapply() or future_lapply() foreach() %dopar% { ... } is just like lapply() or future_lapply() What tricks us, is the %dopar% infix operator. It makes foreach() look like a for-loop, although it isn’t one. If the author of foreach wouldn’t have invented %dopar%, they would probably have written foreach() to work like: res &lt;- foreach(ii = 1:3, FUN = function(ii) { letters[ii] } which would make it clear that foreach() is just another map-reduce function very similar to lapply(). To further bring the message home, it wouldn’t be hard to imagine an implementation of lapply() that could be written as: res &lt;- lapply(ii = 1:3) %dopar% { letters[ii] } I hope that clarifies it. 8.9 Appendix: Debugging For troubleshooting, call backtrace() (sic!), if there is an error when running in parallel. You can also retry with plan(sequential). If you still get an error, then use debug() with plan(sequential, split = TRUE) to interactively step through the problematic function. Since future relay all output, you can also add print(), str(), and message() output to your functions, which is a common poor man’s debugging technique that actually works. 8.9.0.1 For package developers Will my future code work anywhere regardless of where it runs? If the answer is yes, then you’ve embraced the philosophy of futures to 100%. If the answer is no, try to identify exactly what part of the future code won’t work everywhere, and see if it is necessary to have that constrain. It’s always a good practice to never override users settings, including with foreach adapter they might already have registered. For instance, if you do: llply_slow &lt;- function(x) { doFuture::registerDoFuture() llply(x, slow, .parallel = TRUE) } you will break the user’s intentions if they use it as: library(foreach) doParallel::registerDoParallel(2) y1 &lt;- foreach(ii = 1:3) %dopar% { some_other_fcn(ii) } y2 &lt;- llply_slow(1:3) ## here you change the adaptor y3 &lt;- foreach(ii = 3:1) %dopar% { some_other_fcn(ii) } To avoid this, undo your adaptor changes as: llply_slow &lt;- function(x) { oldDoPar &lt;- registerDoFuture() on.exit(with(oldDoPar, foreach::setDoPar(fun=fun, data=data, info=info)), add = TRUE) llply(x, slow, .parallel = TRUE) } "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
